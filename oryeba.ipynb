{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import GRU, Dense, Dropout\n",
    "import keras_tuner as kt\n",
    "from tensorflow.keras.layers import GRU, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"reporte_diario_campaña_limpio.xlsx\").drop(columns='Unnamed: 0')\n",
    "data = df[df[\"linea\"]==\"linea_2\"]\n",
    "\n",
    "# Ordenar por fecha\n",
    "data = data.sort_values('fecha')\n",
    "\n",
    "# Separar características y etiquetas\n",
    "features = ['año', 'mes', \"dia\", 'dia_semana', 'state_holiday']\n",
    "X = data[features]\n",
    "y = data['interpolado_real_calls']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entrenamiento_gru(data,target):\n",
    "    if target == 'interpolado_real_calls':\n",
    "        features = ['año', 'mes', \"dia\", 'dia_semana', 'state_holiday']\n",
    "    else: \n",
    "        features = ['año', 'mes', \"dia\", 'dia_semana', 'state_holiday','interpolado_real_calls']\n",
    "    X = data[features]\n",
    "    y = data['interpolado_real_calls']\n",
    "    scaler_X = StandardScaler()\n",
    "    scaler_y = StandardScaler()\n",
    "\n",
    "    def train_test_split_by_date(data, date_column, train_end_date, test_start_date):\n",
    "        train_data = data[data[date_column] <= train_end_date]\n",
    "        test_data = data[data[date_column] >= test_start_date]\n",
    "        return train_data, test_data\n",
    "    train_data, test_data = train_test_split_by_date(data, 'fecha', '2023-12-31', '2024-01-01')\n",
    "\n",
    "    X_train, y_train = train_data[features], train_data[target]\n",
    "    X_test, y_test = test_data[features], test_data[target]\n",
    "\n",
    "    X_train_scaled = scaler_X.fit_transform(X_train)\n",
    "    y_train_scaled = scaler_y.fit_transform(y_train.values.reshape(-1, 1))\n",
    "\n",
    "    X_test_scaled = scaler_X.transform(X_test)\n",
    "    y_test_scaled = scaler_y.transform(y_test.values.reshape(-1, 1))\n",
    "\n",
    "    def create_sequences(X, y, time_steps=10):\n",
    "        Xs, ys = [], []\n",
    "        for i in range(len(X) - time_steps):\n",
    "            Xs.append(X[i:(i + time_steps)])\n",
    "            ys.append(y[i + time_steps])\n",
    "        return np.array(Xs), np.array(ys)\n",
    "\n",
    "    time_steps = 14\n",
    "    X_train_seq, y_train_seq = create_sequences(X_train_scaled, y_train_scaled, time_steps)\n",
    "    X_test_seq, y_test_seq = create_sequences(X_test_scaled, y_test_scaled, time_steps)\n",
    "    def wape_metric(y_true, y_pred):\n",
    "        return K.sum(K.abs(y_true - y_pred)) / K.sum(K.abs(y_true))\n",
    "    def build_robust_gru_model(hp):\n",
    "        model = Sequential()\n",
    "        \n",
    "        # Capas GRU\n",
    "        for i in range(hp.Int('gru_layers', 1, 3)):\n",
    "            model.add(GRU(units=hp.Int(f'gru_units_{i}', min_value=64, max_value=512, step=64),\n",
    "                        return_sequences=(i < hp.Int('gru_layers', 1, 3) - 1),\n",
    "                        recurrent_dropout=hp.Float(f'recurrent_dropout_{i}', min_value=0.0, max_value=0.3, step=0.1)))\n",
    "            model.add(Dropout(rate=hp.Float(f'dropout_{i}', min_value=0.1, max_value=0.5, step=0.1)))\n",
    "        \n",
    "        # Capa de normalización por lotes\n",
    "        model.add(BatchNormalization())\n",
    "\n",
    "        # Capas densas finales\n",
    "        for i in range(hp.Int('dense_layers', 1, 3)):\n",
    "            model.add(Dense(units=hp.Int(f'dense_units_{i}', min_value=32, max_value=256, step=32), activation='relu'))\n",
    "            model.add(Dropout(rate=hp.Float(f'dense_dropout_{i}', min_value=0.1, max_value=0.5, step=0.1)))\n",
    "\n",
    "        model.add(Dense(1))\n",
    "\n",
    "        # Compilación del modelo\n",
    "        model.compile(optimizer=Adam(hp.Float('learning_rate', min_value=1e-5, max_value=1e-2, sampling='LOG')),\n",
    "                    loss='mse',  # Seguimos utilizando 'mse' para el entrenamiento\n",
    "                    metrics=[wape_metric])  # Pero optimizamos usando WAPE\n",
    "        \n",
    "        return model\n",
    "\n",
    "    tuner = kt.RandomSearch(\n",
    "        build_robust_gru_model,\n",
    "        objective=kt.Objective('val_wape_metric', direction='min'), \n",
    "        max_trials=15,  \n",
    "        executions_per_trial=2, \n",
    "        seed = 47)\n",
    "\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='val_wape_metric', \n",
    "        patience=7, \n",
    "        restore_best_weights=True,\n",
    "        mode = \"min\" \n",
    "    )\n",
    "    # Búsqueda de hiperparámetros\n",
    "    tuner.search(\n",
    "        X_train_seq, \n",
    "        y_train_seq, \n",
    "        epochs=30, \n",
    "        validation_split=0.2, \n",
    "        batch_size=32,\n",
    "        callbacks=[early_stopping] )\n",
    "    best_model = tuner.get_best_models(num_models=1)[0]\n",
    "\n",
    "    best_model.fit(X_train_seq, y_train_seq, epochs=100, validation_split=0.2, batch_size=32)\n",
    "\n",
    "    y_pred_gru_tuned = best_model.predict(X_test_seq)\n",
    "    y_pred_gru_tuned_rescaled = scaler_y.inverse_transform(y_pred_gru_tuned)\n",
    "\n",
    "    return y_pred_gru_tuned_rescaled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizar los datos\n",
    "scaler_X = StandardScaler()\n",
    "X_scaled = scaler_X.fit_transform(X)\n",
    "\n",
    "scaler_y = StandardScaler()\n",
    "y_scaled = scaler_y.fit_transform(y.values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear una función para identificar el conjunto de entrenamiento y prueba\n",
    "def train_test_split_by_date(data, date_column, train_end_date, test_start_date):\n",
    "    train_data = data[data[date_column] <= train_end_date]\n",
    "    test_data = data[data[date_column] >= test_start_date]\n",
    "    return train_data, test_data\n",
    "\n",
    "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "train_data, test_data = train_test_split_by_date(data, 'fecha', '2023-12-31', '2024-01-01')\n",
    "\n",
    "# Separar características y etiquetas para entrenamiento y prueba\n",
    "X_train, y_train = train_data[features], train_data['interpolado_real_calls']\n",
    "X_test, y_test = test_data[features], test_data['interpolado_real_calls']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Escalar los datos\n",
    "X_train_scaled = scaler_X.fit_transform(X_train)\n",
    "y_train_scaled = scaler_y.fit_transform(y_train.values.reshape(-1, 1))\n",
    "\n",
    "X_test_scaled = scaler_X.transform(X_test)\n",
    "y_test_scaled = scaler_y.transform(y_test.values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(X, y, time_steps=10):\n",
    "    Xs, ys = [], []\n",
    "    for i in range(len(X) - time_steps):\n",
    "        Xs.append(X[i:(i + time_steps)])\n",
    "        ys.append(y[i + time_steps])\n",
    "    return np.array(Xs), np.array(ys)\n",
    "\n",
    "time_steps = 10\n",
    "X_train_seq, y_train_seq = create_sequences(X_train_scaled, y_train_scaled, time_steps)\n",
    "X_test_seq, y_test_seq = create_sequences(X_test_scaled, y_test_scaled, time_steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir WAPE como métrica personalizada\n",
    "def wape_metric(y_true, y_pred):\n",
    "    return K.sum(K.abs(y_true - y_pred)) / K.sum(K.abs(y_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_robust_gru_model(hp):\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Capas GRU\n",
    "    for i in range(hp.Int('gru_layers', 1, 3)):\n",
    "        model.add(GRU(units=hp.Int(f'gru_units_{i}', min_value=64, max_value=512, step=64),\n",
    "                      return_sequences=(i < hp.Int('gru_layers', 1, 3) - 1),\n",
    "                      recurrent_dropout=hp.Float(f'recurrent_dropout_{i}', min_value=0.0, max_value=0.3, step=0.1)))\n",
    "        model.add(Dropout(rate=hp.Float(f'dropout_{i}', min_value=0.1, max_value=0.5, step=0.1)))\n",
    "    \n",
    "    # Capa de normalización por lotes\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    # Capas densas finales\n",
    "    for i in range(hp.Int('dense_layers', 1, 3)):\n",
    "        model.add(Dense(units=hp.Int(f'dense_units_{i}', min_value=32, max_value=256, step=32), activation='relu'))\n",
    "        model.add(Dropout(rate=hp.Float(f'dense_dropout_{i}', min_value=0.1, max_value=0.5, step=0.1)))\n",
    "\n",
    "    model.add(Dense(1))\n",
    "\n",
    "    # Compilación del modelo\n",
    "    model.compile(optimizer=Adam(hp.Float('learning_rate', min_value=1e-5, max_value=1e-2, sampling='LOG')),\n",
    "                  loss='mse',  # Seguimos utilizando 'mse' para el entrenamiento\n",
    "                  metrics=[wape_metric])  # Pero optimizamos usando WAPE\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 15 Complete [00h 04m 43s]\n",
      "val_wape_metric: 0.39876797795295715\n",
      "\n",
      "Best val_wape_metric So Far: 0.3517134338617325\n",
      "Total elapsed time: 00h 46m 20s\n"
     ]
    }
   ],
   "source": [
    "# Definición del tuner para minimizar WAPE\n",
    "tuner = kt.RandomSearch(\n",
    "    build_robust_gru_model,\n",
    "    objective=kt.Objective('val_wape_metric', direction='min'), \n",
    "    max_trials=15,  \n",
    "    executions_per_trial=2, \n",
    "    seed = 47)\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_wape_metric', \n",
    "    patience=7, \n",
    "    restore_best_weights=True,\n",
    "    mode = \"min\" \n",
    ")\n",
    "# Búsqueda de hiperparámetros\n",
    "tuner.search(\n",
    "    X_train_seq, \n",
    "    y_train_seq, \n",
    "    epochs=30, \n",
    "    validation_split=0.2, \n",
    "    batch_size=32,\n",
    "    callbacks=[early_stopping]  # Agregamos el callback de Early Stopping\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LauraVanessaRocaOroz\\Desktop\\TFM\\tfm_venv\\Lib\\site-packages\\keras\\src\\saving\\saving_lib.py:713: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 36 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 164ms/step - loss: 0.1962 - wape_metric: 0.3323 - val_loss: 0.2176 - val_wape_metric: 0.5909\n",
      "Epoch 2/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 128ms/step - loss: 0.2095 - wape_metric: 0.3325 - val_loss: 0.1492 - val_wape_metric: 0.4393\n",
      "Epoch 3/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 110ms/step - loss: 0.2009 - wape_metric: 0.3337 - val_loss: 0.1114 - val_wape_metric: 0.3904\n",
      "Epoch 4/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 113ms/step - loss: 0.1950 - wape_metric: 0.3417 - val_loss: 0.2064 - val_wape_metric: 0.5936\n",
      "Epoch 5/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 119ms/step - loss: 0.1920 - wape_metric: 0.3140 - val_loss: 0.1693 - val_wape_metric: 0.5124\n",
      "Epoch 6/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 113ms/step - loss: 0.1729 - wape_metric: 0.3094 - val_loss: 0.1269 - val_wape_metric: 0.4231\n",
      "Epoch 7/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 142ms/step - loss: 0.1843 - wape_metric: 0.3290 - val_loss: 0.1077 - val_wape_metric: 0.3786\n",
      "Epoch 8/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 100ms/step - loss: 0.1807 - wape_metric: 0.2985 - val_loss: 0.1786 - val_wape_metric: 0.5204\n",
      "Epoch 9/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 106ms/step - loss: 0.1806 - wape_metric: 0.3034 - val_loss: 0.1490 - val_wape_metric: 0.4740\n",
      "Epoch 10/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 120ms/step - loss: 0.1736 - wape_metric: 0.3005 - val_loss: 0.1417 - val_wape_metric: 0.4501\n",
      "Epoch 11/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 129ms/step - loss: 0.1743 - wape_metric: 0.2948 - val_loss: 0.1178 - val_wape_metric: 0.3934\n",
      "Epoch 12/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 112ms/step - loss: 0.1529 - wape_metric: 0.2693 - val_loss: 0.1340 - val_wape_metric: 0.4359\n",
      "Epoch 13/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 145ms/step - loss: 0.1606 - wape_metric: 0.2867 - val_loss: 0.1201 - val_wape_metric: 0.4075\n",
      "Epoch 14/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 130ms/step - loss: 0.1757 - wape_metric: 0.3006 - val_loss: 0.1266 - val_wape_metric: 0.4189\n",
      "Epoch 15/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 118ms/step - loss: 0.1531 - wape_metric: 0.2812 - val_loss: 0.1319 - val_wape_metric: 0.4158\n",
      "Epoch 16/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 119ms/step - loss: 0.1515 - wape_metric: 0.2831 - val_loss: 0.1733 - val_wape_metric: 0.5220\n",
      "Epoch 17/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 133ms/step - loss: 0.1428 - wape_metric: 0.2760 - val_loss: 0.1398 - val_wape_metric: 0.4635\n",
      "Epoch 18/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 134ms/step - loss: 0.1716 - wape_metric: 0.2853 - val_loss: 0.1396 - val_wape_metric: 0.4569\n",
      "Epoch 19/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 118ms/step - loss: 0.2028 - wape_metric: 0.3088 - val_loss: 0.1091 - val_wape_metric: 0.3767\n",
      "Epoch 20/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 116ms/step - loss: 0.2107 - wape_metric: 0.3315 - val_loss: 0.3445 - val_wape_metric: 0.7138\n",
      "Epoch 21/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 122ms/step - loss: 0.2469 - wape_metric: 0.3828 - val_loss: 0.1416 - val_wape_metric: 0.4583\n",
      "Epoch 22/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 125ms/step - loss: 0.1753 - wape_metric: 0.2926 - val_loss: 0.1404 - val_wape_metric: 0.4664\n",
      "Epoch 23/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 137ms/step - loss: 0.1395 - wape_metric: 0.2697 - val_loss: 0.2456 - val_wape_metric: 0.5885\n",
      "Epoch 24/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 120ms/step - loss: 0.2320 - wape_metric: 0.3404 - val_loss: 0.1721 - val_wape_metric: 0.5204\n",
      "Epoch 25/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 132ms/step - loss: 0.1531 - wape_metric: 0.2801 - val_loss: 0.2071 - val_wape_metric: 0.5678\n",
      "Epoch 26/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 124ms/step - loss: 0.1666 - wape_metric: 0.2746 - val_loss: 0.1653 - val_wape_metric: 0.5049\n",
      "Epoch 27/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 137ms/step - loss: 0.1435 - wape_metric: 0.2515 - val_loss: 0.1838 - val_wape_metric: 0.5492\n",
      "Epoch 28/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 126ms/step - loss: 0.1553 - wape_metric: 0.2702 - val_loss: 0.1640 - val_wape_metric: 0.5179\n",
      "Epoch 29/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 134ms/step - loss: 0.1759 - wape_metric: 0.2896 - val_loss: 0.1661 - val_wape_metric: 0.5185\n",
      "Epoch 30/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 111ms/step - loss: 0.1675 - wape_metric: 0.2863 - val_loss: 0.1935 - val_wape_metric: 0.5534\n",
      "Epoch 31/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 110ms/step - loss: 0.1532 - wape_metric: 0.2701 - val_loss: 0.1499 - val_wape_metric: 0.4867\n",
      "Epoch 32/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 129ms/step - loss: 0.1579 - wape_metric: 0.2782 - val_loss: 0.1271 - val_wape_metric: 0.4327\n",
      "Epoch 33/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 123ms/step - loss: 0.1468 - wape_metric: 0.2730 - val_loss: 0.1535 - val_wape_metric: 0.4885\n",
      "Epoch 34/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 128ms/step - loss: 0.1147 - wape_metric: 0.2303 - val_loss: 0.1570 - val_wape_metric: 0.5062\n",
      "Epoch 35/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 111ms/step - loss: 0.1048 - wape_metric: 0.2251 - val_loss: 0.1299 - val_wape_metric: 0.4310\n",
      "Epoch 36/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 102ms/step - loss: 0.1560 - wape_metric: 0.2787 - val_loss: 0.1859 - val_wape_metric: 0.5812\n",
      "Epoch 37/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 105ms/step - loss: 0.1226 - wape_metric: 0.2534 - val_loss: 0.1477 - val_wape_metric: 0.4910\n",
      "Epoch 38/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 108ms/step - loss: 0.1405 - wape_metric: 0.2540 - val_loss: 0.1332 - val_wape_metric: 0.4488\n",
      "Epoch 39/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 110ms/step - loss: 0.1385 - wape_metric: 0.2611 - val_loss: 0.1159 - val_wape_metric: 0.3934\n",
      "Epoch 40/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 109ms/step - loss: 0.1453 - wape_metric: 0.2617 - val_loss: 0.1403 - val_wape_metric: 0.4719\n",
      "Epoch 41/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 104ms/step - loss: 0.0964 - wape_metric: 0.2234 - val_loss: 0.1226 - val_wape_metric: 0.4235\n",
      "Epoch 42/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 134ms/step - loss: 0.1156 - wape_metric: 0.2317 - val_loss: 0.1635 - val_wape_metric: 0.5173\n",
      "Epoch 43/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 128ms/step - loss: 0.1213 - wape_metric: 0.2427 - val_loss: 0.1450 - val_wape_metric: 0.4730\n",
      "Epoch 44/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 127ms/step - loss: 0.1243 - wape_metric: 0.2367 - val_loss: 0.1289 - val_wape_metric: 0.4347\n",
      "Epoch 45/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 130ms/step - loss: 0.1241 - wape_metric: 0.2394 - val_loss: 0.1907 - val_wape_metric: 0.5790\n",
      "Epoch 46/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 124ms/step - loss: 0.1221 - wape_metric: 0.2340 - val_loss: 0.1720 - val_wape_metric: 0.5433\n",
      "Epoch 47/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 101ms/step - loss: 0.0891 - wape_metric: 0.2172 - val_loss: 0.1541 - val_wape_metric: 0.5010\n",
      "Epoch 48/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 113ms/step - loss: 0.1009 - wape_metric: 0.2194 - val_loss: 0.1611 - val_wape_metric: 0.5186\n",
      "Epoch 49/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 127ms/step - loss: 0.1115 - wape_metric: 0.2347 - val_loss: 0.1370 - val_wape_metric: 0.4175\n",
      "Epoch 50/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 128ms/step - loss: 0.1143 - wape_metric: 0.2267 - val_loss: 0.1392 - val_wape_metric: 0.4633\n",
      "Epoch 51/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 127ms/step - loss: 0.1278 - wape_metric: 0.2362 - val_loss: 0.1421 - val_wape_metric: 0.4634\n",
      "Epoch 52/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 116ms/step - loss: 0.0985 - wape_metric: 0.2155 - val_loss: 0.1756 - val_wape_metric: 0.5393\n",
      "Epoch 53/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 119ms/step - loss: 0.1233 - wape_metric: 0.2282 - val_loss: 0.1380 - val_wape_metric: 0.4388\n",
      "Epoch 54/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 115ms/step - loss: 0.0888 - wape_metric: 0.2100 - val_loss: 0.1257 - val_wape_metric: 0.4268\n",
      "Epoch 55/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 133ms/step - loss: 0.0821 - wape_metric: 0.2062 - val_loss: 0.1114 - val_wape_metric: 0.3712\n",
      "Epoch 56/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 120ms/step - loss: 0.1122 - wape_metric: 0.2377 - val_loss: 0.1596 - val_wape_metric: 0.4988\n",
      "Epoch 57/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 117ms/step - loss: 0.1196 - wape_metric: 0.2229 - val_loss: 0.1207 - val_wape_metric: 0.4030\n",
      "Epoch 58/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 163ms/step - loss: 0.1011 - wape_metric: 0.2161 - val_loss: 0.1418 - val_wape_metric: 0.4686\n",
      "Epoch 59/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 101ms/step - loss: 0.0941 - wape_metric: 0.2073 - val_loss: 0.1334 - val_wape_metric: 0.4301\n",
      "Epoch 60/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 104ms/step - loss: 0.0765 - wape_metric: 0.1857 - val_loss: 0.1154 - val_wape_metric: 0.3974\n",
      "Epoch 61/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 90ms/step - loss: 0.0974 - wape_metric: 0.2118 - val_loss: 0.1126 - val_wape_metric: 0.3719\n",
      "Epoch 62/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 90ms/step - loss: 0.1158 - wape_metric: 0.2270 - val_loss: 0.1658 - val_wape_metric: 0.4851\n",
      "Epoch 63/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 93ms/step - loss: 0.1050 - wape_metric: 0.2282 - val_loss: 0.1318 - val_wape_metric: 0.4288\n",
      "Epoch 64/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 92ms/step - loss: 0.0889 - wape_metric: 0.1981 - val_loss: 0.1326 - val_wape_metric: 0.4284\n",
      "Epoch 65/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 92ms/step - loss: 0.0837 - wape_metric: 0.1970 - val_loss: 0.1156 - val_wape_metric: 0.3860\n",
      "Epoch 66/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 92ms/step - loss: 0.0966 - wape_metric: 0.2053 - val_loss: 0.1036 - val_wape_metric: 0.3522\n",
      "Epoch 67/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 91ms/step - loss: 0.1063 - wape_metric: 0.2139 - val_loss: 0.1176 - val_wape_metric: 0.3921\n",
      "Epoch 68/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 96ms/step - loss: 0.1000 - wape_metric: 0.2094 - val_loss: 0.1461 - val_wape_metric: 0.4532\n",
      "Epoch 69/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 94ms/step - loss: 0.0876 - wape_metric: 0.1962 - val_loss: 0.1343 - val_wape_metric: 0.4328\n",
      "Epoch 70/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 87ms/step - loss: 0.0892 - wape_metric: 0.2049 - val_loss: 0.1308 - val_wape_metric: 0.4151\n",
      "Epoch 71/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 89ms/step - loss: 0.1060 - wape_metric: 0.2111 - val_loss: 0.1373 - val_wape_metric: 0.4496\n",
      "Epoch 72/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 91ms/step - loss: 0.1293 - wape_metric: 0.2246 - val_loss: 0.1544 - val_wape_metric: 0.4858\n",
      "Epoch 73/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 93ms/step - loss: 0.0977 - wape_metric: 0.1975 - val_loss: 0.1278 - val_wape_metric: 0.4125\n",
      "Epoch 74/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 111ms/step - loss: 0.0843 - wape_metric: 0.1854 - val_loss: 0.1238 - val_wape_metric: 0.4166\n",
      "Epoch 75/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 117ms/step - loss: 0.0861 - wape_metric: 0.1977 - val_loss: 0.1285 - val_wape_metric: 0.4148\n",
      "Epoch 76/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 101ms/step - loss: 0.0971 - wape_metric: 0.2100 - val_loss: 0.1280 - val_wape_metric: 0.4051\n",
      "Epoch 77/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 89ms/step - loss: 0.0913 - wape_metric: 0.1999 - val_loss: 0.1196 - val_wape_metric: 0.3740\n",
      "Epoch 78/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 95ms/step - loss: 0.0689 - wape_metric: 0.1925 - val_loss: 0.1070 - val_wape_metric: 0.3797\n",
      "Epoch 79/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 115ms/step - loss: 0.0984 - wape_metric: 0.2000 - val_loss: 0.1540 - val_wape_metric: 0.4622\n",
      "Epoch 80/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 109ms/step - loss: 0.0678 - wape_metric: 0.1791 - val_loss: 0.1666 - val_wape_metric: 0.4814\n",
      "Epoch 81/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 123ms/step - loss: 0.0994 - wape_metric: 0.1975 - val_loss: 0.1523 - val_wape_metric: 0.4755\n",
      "Epoch 82/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 98ms/step - loss: 0.1001 - wape_metric: 0.2025 - val_loss: 0.2219 - val_wape_metric: 0.5379\n",
      "Epoch 83/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 96ms/step - loss: 0.1110 - wape_metric: 0.2039 - val_loss: 0.1439 - val_wape_metric: 0.4230\n",
      "Epoch 84/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 97ms/step - loss: 0.1023 - wape_metric: 0.1977 - val_loss: 0.1239 - val_wape_metric: 0.3992\n",
      "Epoch 85/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 100ms/step - loss: 0.0725 - wape_metric: 0.1864 - val_loss: 0.1484 - val_wape_metric: 0.4828\n",
      "Epoch 86/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 110ms/step - loss: 0.1081 - wape_metric: 0.2101 - val_loss: 0.1429 - val_wape_metric: 0.4511\n",
      "Epoch 87/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 123ms/step - loss: 0.1120 - wape_metric: 0.2152 - val_loss: 0.1247 - val_wape_metric: 0.3932\n",
      "Epoch 88/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 98ms/step - loss: 0.1026 - wape_metric: 0.2030 - val_loss: 0.1288 - val_wape_metric: 0.3874\n",
      "Epoch 89/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 107ms/step - loss: 0.0865 - wape_metric: 0.1983 - val_loss: 0.1339 - val_wape_metric: 0.4370\n",
      "Epoch 90/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 101ms/step - loss: 0.0788 - wape_metric: 0.1848 - val_loss: 0.1432 - val_wape_metric: 0.4414\n",
      "Epoch 91/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 108ms/step - loss: 0.0826 - wape_metric: 0.2028 - val_loss: 0.1132 - val_wape_metric: 0.3563\n",
      "Epoch 92/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 100ms/step - loss: 0.0868 - wape_metric: 0.2046 - val_loss: 0.1441 - val_wape_metric: 0.4246\n",
      "Epoch 93/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 124ms/step - loss: 0.0664 - wape_metric: 0.1801 - val_loss: 0.1201 - val_wape_metric: 0.4008\n",
      "Epoch 94/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 118ms/step - loss: 0.0632 - wape_metric: 0.1774 - val_loss: 0.1271 - val_wape_metric: 0.4034\n",
      "Epoch 95/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 99ms/step - loss: 0.1735 - wape_metric: 0.2745 - val_loss: 0.2713 - val_wape_metric: 0.5850\n",
      "Epoch 96/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 125ms/step - loss: 0.1591 - wape_metric: 0.2742 - val_loss: 0.2511 - val_wape_metric: 0.6408\n",
      "Epoch 97/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 122ms/step - loss: 0.1304 - wape_metric: 0.2261 - val_loss: 0.1487 - val_wape_metric: 0.4739\n",
      "Epoch 98/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 104ms/step - loss: 0.1075 - wape_metric: 0.2141 - val_loss: 0.1495 - val_wape_metric: 0.4678\n",
      "Epoch 99/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 110ms/step - loss: 0.0891 - wape_metric: 0.1906 - val_loss: 0.1196 - val_wape_metric: 0.4167\n",
      "Epoch 100/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 116ms/step - loss: 0.0996 - wape_metric: 0.2091 - val_loss: 0.1592 - val_wape_metric: 0.5027\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 673ms/step\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found array with dim 3. None expected <= 2.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m y_pred_gru_tuned \u001b[38;5;241m=\u001b[39m best_model\u001b[38;5;241m.\u001b[39mpredict(X_test_seq)\n\u001b[0;32m      9\u001b[0m y_pred_gru_tuned_rescaled \u001b[38;5;241m=\u001b[39m scaler_y\u001b[38;5;241m.\u001b[39minverse_transform(y_pred_gru_tuned)\n\u001b[1;32m---> 10\u001b[0m y_test_rescaled\u001b[38;5;241m=\u001b[39m \u001b[43mscaler_y\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minverse_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test_seq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Cálculo de WAPE para el modelo ajustado\u001b[39;00m\n\u001b[0;32m     13\u001b[0m wape_gru_tuned \u001b[38;5;241m=\u001b[39m wape_metric(y_test_rescaled, y_pred_gru_tuned_rescaled)\n",
      "File \u001b[1;32mc:\\Users\\LauraVanessaRocaOroz\\Desktop\\TFM\\tfm_venv\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py:1085\u001b[0m, in \u001b[0;36mStandardScaler.inverse_transform\u001b[1;34m(self, X, copy)\u001b[0m\n\u001b[0;32m   1082\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m   1084\u001b[0m copy \u001b[38;5;241m=\u001b[39m copy \u001b[38;5;28;01mif\u001b[39;00m copy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy\n\u001b[1;32m-> 1085\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1086\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1087\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1088\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1089\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mFLOAT_DTYPES\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1090\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1091\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1093\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sparse\u001b[38;5;241m.\u001b[39missparse(X):\n\u001b[0;32m   1094\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwith_mean:\n",
      "File \u001b[1;32mc:\\Users\\LauraVanessaRocaOroz\\Desktop\\TFM\\tfm_venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:1043\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m   1038\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1039\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumeric\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is not compatible with arrays of bytes/strings.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1040\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConvert your data to numeric values explicitly instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1041\u001b[0m     )\n\u001b[0;32m   1042\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m allow_nd \u001b[38;5;129;01mand\u001b[39;00m array\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m:\n\u001b[1;32m-> 1043\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1044\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1045\u001b[0m         \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n\u001b[0;32m   1046\u001b[0m     )\n\u001b[0;32m   1048\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m force_all_finite:\n\u001b[0;32m   1049\u001b[0m     _assert_all_finite(\n\u001b[0;32m   1050\u001b[0m         array,\n\u001b[0;32m   1051\u001b[0m         input_name\u001b[38;5;241m=\u001b[39minput_name,\n\u001b[0;32m   1052\u001b[0m         estimator_name\u001b[38;5;241m=\u001b[39mestimator_name,\n\u001b[0;32m   1053\u001b[0m         allow_nan\u001b[38;5;241m=\u001b[39mforce_all_finite \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow-nan\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1054\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Found array with dim 3. None expected <= 2."
     ]
    }
   ],
   "source": [
    "# Obtener el mejor modelo basado en WAPE\n",
    "best_model = tuner.get_best_models(num_models=1)[0]\n",
    "\n",
    "# Entrenamiento adicional del mejor modelo si es necesario\n",
    "best_model.fit(X_train_seq, y_train_seq, epochs=100, validation_split=0.2, batch_size=32)\n",
    "\n",
    "# Evaluación del mejor modelo\n",
    "y_pred_gru_tuned = best_model.predict(X_test_seq)\n",
    "y_pred_gru_tuned_rescaled = scaler_y.inverse_transform(y_pred_gru_tuned)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WAPE GRU Tuned: 0.13%\n"
     ]
    }
   ],
   "source": [
    "y_test_rescaled= scaler_y.inverse_transform(y_test_seq)\n",
    "\n",
    "# Cálculo de WAPE para el modelo ajustado\n",
    "wape_gru_tuned = wape_metric(y_test_rescaled, y_pred_gru_tuned_rescaled)\n",
    "\n",
    "# Mostrar el resultado de WAPE\n",
    "print(f\"WAPE GRU Tuned: {wape_gru_tuned:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfm_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
