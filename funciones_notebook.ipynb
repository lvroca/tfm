{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importación librerías\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pycaret.regression import *\n",
    "from pycaret.time_series import *\n",
    "from prophet import Prophet\n",
    "from scipy.stats import uniform\n",
    "from mango import Tuner\n",
    "import json\n",
    "import tensorflow\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tfkeras.models import Sequential\n",
    "from keras.layers import GRU, Dense, Dropout,BatchNormalization\n",
    "import keras_tuner as kt\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as K\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logger = logging.getLogger('cmdstanpy')\n",
    "logger.addHandler(logging.NullHandler())\n",
    "logger.propagate = False\n",
    "logger.setLevel(logging.CRITICAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lectura de datos\n",
    "df = pd.read_excel(\"reporte_diario_campaña_limpio.xlsx\").drop(columns=\"Unnamed: 0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definición error \n",
    "def wape(y,y_pred,**kwargs):\n",
    "    wape = np.sum(np.abs(y - y_pred)) / np.sum(y)\n",
    "    return wape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenamiento Pycaret Regresión\n",
    "def entrenamiento_pycaret_reg(datos_entrenamiento, datos_prueba,target):\n",
    "    reg = RegressionExperiment()\n",
    "    reg.setup(datos_entrenamiento, target=target, session_id=47, preprocess=False, verbose=False,\n",
    "              fold=3,system_log=False,memory=False,data_split_shuffle=False,fold_strategy=\"timeseries\"),\n",
    "    reg.add_metric('wape', 'WAPE', wape, greater_is_better=False)\n",
    "    best_reg = reg.compare_models(sort=\"WAPE\", verbose=False, fold = 3)\n",
    "    best_model_reg = reg.create_model(best_reg,fold=3,verbose = False)\n",
    "    tuned_reg = reg.tune_model(best_model_reg,verbose=False, fold=3,optimize = \"WAPE\")\n",
    "    pred_reg = reg.predict_model(tuned_reg, data=datos_prueba[['año', 'mes', \"dia\", 'dia_semana', 'state_holiday']])\n",
    "    return pred_reg, best_reg,tuned_reg.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenamiento Pycaret Time series\n",
    "def entrenamiento_pycaret_ts(datos_entrenamiento,datos_prueba,n_periodos,target,fold):\n",
    "    ts= TSForecastingExperiment()\n",
    "    ts.setup(datos_entrenamiento, target=target, session_id=47,fh=n_periodos,  verbose=False,fold_strategy=\"rolling\",fold=fold)\n",
    "    best_ts = ts.compare_models(sort=\"MAPE\", verbose=False)\n",
    "    best_model_ts = ts.create_model(best_ts,verbose = False)\n",
    "    tuned_ts = ts.tune_model(best_model_ts,verbose=False,optimize = \"MAPE\")\n",
    "    pred_ts = ts.predict_model(tuned_ts, X=datos_prueba[['año', 'mes', \"dia\", 'dia_semana', 'state_holiday']], fh=n_periodos, verbose=False)\n",
    "    \n",
    "    return pred_ts, best_ts,tuned_ts.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenamiento Prophet\n",
    "def entrenamiento_prophet(datos, holidays, n_periodos):\n",
    "\n",
    "    n = len(datos[:-n_periodos])\n",
    "    n_train = int(n * 0.8)  \n",
    "    train_df = datos[:n_train]\n",
    "    val_df = datos[n_train:]\n",
    "    test_df = datos[-n_periodos:]\n",
    "    \n",
    "    param_space = dict(\n",
    "        growth=['linear', 'flat'],\n",
    "        n_changepoints=range(0, 55, 5),\n",
    "        changepoint_range=uniform(0.5, 0.5),\n",
    "        yearly_seasonality=[True, False],\n",
    "        weekly_seasonality=[True, False],\n",
    "        daily_seasonality=[True, False],\n",
    "        seasonality_mode=['additive', 'multiplicative'],\n",
    "        seasonality_prior_scale=uniform(5.0, 15.0),\n",
    "        changepoint_prior_scale=uniform(0.0, 0.1),\n",
    "        interval_width=uniform(0.2, 0.8),\n",
    "        uncertainty_samples=[500, 1000, 1500, 2000]\n",
    "    )\n",
    "    \n",
    "    def objective_function(args_list, train_df, val_df):\n",
    "        results = []\n",
    "        \n",
    "        for params in args_list:\n",
    "            model = Prophet(holidays=holidays, **params)\n",
    "            model.fit(train_df)\n",
    "            future = model.make_future_dataframe(periods=len(val_df), freq='D')\n",
    "            forecast = model.predict(future)\n",
    "            predictions_tuned = forecast.tail(len(val_df))\n",
    "            error = wape(val_df['y'], predictions_tuned['yhat'])   \n",
    "            results.append(error)\n",
    "            \n",
    "        return results\n",
    "\n",
    "    conf_Dict = dict()\n",
    "    conf_Dict['initial_random'] = 10\n",
    "    conf_Dict['num_iteration'] = 50\n",
    "\n",
    "    tuner = Tuner(param_space, lambda x: objective_function(x, train_df, test_df), conf_Dict)\n",
    "    results = tuner.minimize()\n",
    "\n",
    "    model = Prophet(holidays=holidays, **results['best_params'])\n",
    "    model.fit(train_df)\n",
    "    future = model.make_future_dataframe(periods=len(test_df), freq='D')\n",
    "    forecast = model.predict(future)\n",
    "    \n",
    "    return forecast['yhat'],results['best_params']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenamiento GRU\n",
    "def entrenamiento_gru(data,target,linea):\n",
    "    if target == 'interpolado_real_calls':\n",
    "        features = ['año', 'mes', \"dia\", 'dia_semana', 'state_holiday']\n",
    "    else: \n",
    "        features = ['año', 'mes', \"dia\", 'dia_semana', 'state_holiday','interpolado_real_calls']\n",
    "    X = data[features]\n",
    "    y = data[target]\n",
    "    scaler_X = StandardScaler()\n",
    "    scaler_y = StandardScaler()\n",
    "\n",
    "    def train_test_split_by_date(data, date_column, train_end_date, test_start_date, time_step):\n",
    "        data[date_column] = pd.to_datetime(data[date_column])\n",
    "\n",
    "        adjusted_test_start_date = pd.to_datetime(test_start_date) - pd.Timedelta(days=time_step)\n",
    "\n",
    "        train_data = data[data[date_column] <= train_end_date]\n",
    "        test_data = data[data[date_column] >= adjusted_test_start_date]\n",
    "        \n",
    "        return train_data, test_data\n",
    "    train_data, test_data = train_test_split_by_date(data, 'fecha', '2023-12-31', '2024-01-01',14)\n",
    "\n",
    "    X_train, y_train = train_data[features], train_data[target]\n",
    "    X_test, y_test = test_data[features], test_data[target]\n",
    "\n",
    "    X_train_scaled = scaler_X.fit_transform(X_train)\n",
    "    y_train_scaled = scaler_y.fit_transform(y_train.values.reshape(-1, 1))\n",
    "\n",
    "    X_test_scaled = scaler_X.transform(X_test)\n",
    "    y_test_scaled = scaler_y.transform(y_test.values.reshape(-1, 1))\n",
    "\n",
    "    def create_sequences(X, y, time_steps=10):\n",
    "        Xs, ys = [], []\n",
    "        for i in range(len(X) - time_steps):\n",
    "            Xs.append(X[i:(i + time_steps)])\n",
    "            ys.append(y[i + time_steps])\n",
    "        return np.array(Xs), np.array(ys)\n",
    "\n",
    "    time_steps = 14\n",
    "    X_train_seq, y_train_seq = create_sequences(X_train_scaled, y_train_scaled, time_steps)\n",
    "    X_test_seq, y_test_seq = create_sequences(X_test_scaled, y_test_scaled, time_steps)\n",
    "    def wape_metric(y_true, y_pred):\n",
    "        return K.sum(K.abs(y_true - y_pred)) / K.sum(K.abs(y_true))\n",
    "    def build_robust_gru_model(hp):\n",
    "        model = Sequential()\n",
    "        \n",
    "        # Capas GRU\n",
    "        for i in range(hp.Int('gru_layers', 1, 3)):\n",
    "            model.add(GRU(units=hp.Int(f'gru_units_{i}', min_value=64, max_value=512, step=64),\n",
    "                        return_sequences=(i < hp.Int('gru_layers', 1, 3) - 1),\n",
    "                        recurrent_dropout=hp.Float(f'recurrent_dropout_{i}', min_value=0.0, max_value=0.3, step=0.1)))\n",
    "            model.add(Dropout(rate=hp.Float(f'dropout_{i}', min_value=0.1, max_value=0.5, step=0.1)))\n",
    "        \n",
    "        # Capa de normalización por lotes\n",
    "        model.add(BatchNormalization())\n",
    "\n",
    "        # Capas densas finales\n",
    "        for i in range(hp.Int('dense_layers', 1, 3)):\n",
    "            model.add(Dense(units=hp.Int(f'dense_units_{i}', min_value=32, max_value=256, step=32), activation='relu'))\n",
    "            model.add(Dropout(rate=hp.Float(f'dense_dropout_{i}', min_value=0.1, max_value=0.5, step=0.1)))\n",
    "\n",
    "        model.add(Dense(1))\n",
    "\n",
    "        # Compilación del modelo\n",
    "        model.compile(optimizer=Adam(hp.Float('learning_rate', min_value=1e-5, max_value=1e-2, sampling='LOG')),\n",
    "                    loss='mse',  # Seguimos utilizando 'mse' para el entrenamiento\n",
    "                    metrics=[wape_metric])  # Pero optimizamos usando WAPE\n",
    "        \n",
    "        return model\n",
    "\n",
    "    tuner = kt.RandomSearch(\n",
    "        build_robust_gru_model,\n",
    "        objective=kt.Objective('val_wape_metric', direction='min'), \n",
    "        max_trials=15,  \n",
    "        executions_per_trial=2, \n",
    "        seed = 47,\n",
    "        directory=f'gru_tuning_{linea}_{target}',\n",
    "        project_name=f'gru_tuning_timeseries_{linea}_{target}')\n",
    "\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='val_wape_metric', \n",
    "        patience=7, \n",
    "        restore_best_weights=True,\n",
    "        mode = \"min\" \n",
    "    )\n",
    "    # Búsqueda de hiperparámetros\n",
    "    tuner.search(\n",
    "        X_train_seq, \n",
    "        y_train_seq, \n",
    "        epochs=30, \n",
    "        validation_split=0.2, \n",
    "        batch_size=32,\n",
    "        callbacks=[early_stopping] )\n",
    "    best_model = tuner.get_best_models(num_models=1)[0]\n",
    "\n",
    "    best_model.fit(X_train_seq, y_train_seq, epochs=100, validation_split=0.2, batch_size=32)\n",
    "\n",
    "    y_pred_gru_tuned = best_model.predict(X_test_seq)\n",
    "    y_pred_gru_tuned_rescaled = scaler_y.inverse_transform(y_pred_gru_tuned)\n",
    "\n",
    "    return y_pred_gru_tuned_rescaled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validacion_multiple(dfs, target, linea):\n",
    "\n",
    "    \"\"\"\n",
    "    Realiza una validación múltiple utilizando diferentes enfoques de predicción (Pycaret Regresión, Pycaret Series Temporales, Facebook Prophet y Promedio Móvil) \n",
    "\n",
    "    Args:\n",
    "        dfs (pd.DataFrame): DataFrame con los datos históricos\n",
    "        target (str): Nombre de la columna objetivo a predecir\n",
    "        linea (str): Nombre de la línea específica a analizar\n",
    "\n",
    "    Returns:\n",
    "        dict: Un diccionario que contiene el error mínimo, el modelo que lo obtuvo, un DataFrame con los errores de cada modelo, \n",
    "              un DataFrame con las predicciones diarias y los mejores modelos ajustados por pycaret.\n",
    "    \"\"\"\n",
    "    \n",
    "    datos = dfs[dfs[\"linea\"] == linea]\n",
    "    datos = datos.sort_values(by=[\"linea\", \"fecha\", \"año\", \"mes\", \"dia_semana\"])\n",
    "    datos = datos.reset_index(drop=True)\n",
    "\n",
    "    n_periodos=datos[datos['fecha'].dt.year == 2024].shape[0] # Datos del 2024\n",
    "    \n",
    "    datos_red = datos.copy()\n",
    "    datos_prophet = datos[[\"fecha\", target]].rename(columns={\"fecha\": 'ds', target: 'y'})\n",
    "    state_holidays = datos[datos['state_holiday'] == 1][\"fecha\"].unique()\n",
    "    state_holidays = pd.DataFrame({'ds': pd.to_datetime(state_holidays), 'holiday': 'state_holiday'})\n",
    "\n",
    "    if target == \"interpolado_real_calls\":\n",
    "        datos = datos[['año', 'mes', \"dia\", 'dia_semana', target, 'state_holiday']].dropna()\n",
    "    else: \n",
    "        datos = datos[['año', 'mes', \"dia\", 'dia_semana', target, 'state_holiday',\"interpolado_real_calls\"]].dropna()\n",
    "    datos_entrenamiento = datos[:-n_periodos]\n",
    "    datos_prueba = datos[-n_periodos:]\n",
    "    news = datos_prueba.copy()\n",
    "\n",
    "    if linea == \"linea_4\":\n",
    "        fold =2\n",
    "    else:\n",
    "        fold = 3\n",
    "    # Entrenamiento con regresión\n",
    "    pred_reg, mejor_modelo_reg, parametros_reg = entrenamiento_pycaret_reg(datos_entrenamiento, datos_prueba,target)\n",
    "    news[\"pred_regr\"] = pred_reg[\"prediction_label\"]\n",
    "    \n",
    "    # Entrenamiento con Time Series\n",
    "    pred_ts, mejor_modelo_ts, parametros_ts = entrenamiento_pycaret_ts(datos_entrenamiento,datos_prueba,n_periodos,target,fold)\n",
    "    news[\"pred_ts\"] = pred_ts['y_pred']\n",
    "    \n",
    "    # Promedio reg-ts\n",
    "    news[\"pred_promedio\"] = (news[\"pred_regr\"] + news[\"pred_ts\"]) / 2\n",
    "    news = news.reset_index(drop=True)\n",
    "    \n",
    "    # Entrenamiento con Prophet\n",
    "    pronostico, mejores_parametros_prophet= entrenamiento_prophet(datos_prophet, state_holidays, n_periodos)\n",
    "    news[\"pred_fb\"] = pronostico[-n_periodos:].reset_index(drop=True)\n",
    "\n",
    "    # Entrenamiento con gru\n",
    "    pred_gru = entrenamiento_gru(datos_red,target,linea)\n",
    "    news[\"pred_gru\"] = pred_gru\n",
    "\n",
    "\n",
    "    news = pd.merge(news, df[['año', 'mes', \"dia\", \"fecha\", target]], how='left').dropna()\n",
    "    news = news.sort_values(['año', 'mes', \"dia\", \"fecha\", target])\n",
    "    \n",
    "    # Eliminar valores negativos\n",
    "    for i in range(8):\n",
    "        min_target = datos_entrenamiento.groupby([\"dia_semana\"]).agg({target: \"min\"})[target][i]\n",
    "        news.loc[(news[\"pred_ts\"] < 0) & (news[\"dia_semana\"] == i), \"pred_ts\"] = min_target\n",
    "        news.loc[(news[\"pred_fb\"] < 0) & (news[\"dia_semana\"] == i), \"pred_fb\"] = min_target\n",
    "        news.loc[(news[\"pred_regr\"] < 0) & (news[\"dia_semana\"] == i), \"pred_regr\"] = min_target\n",
    "        news.loc[(news[\"pred_promedio\"] < 0) & (news[\"dia_semana\"] == i), \"pred_promedio\"] = min_target\n",
    "        news.loc[(news[\"pred_gru\"] < 0) & (news[\"dia_semana\"] == i), \"pred_gru\"] = min_target\n",
    "    \n",
    "\n",
    "    news[\"linea\"] = linea\n",
    "    \n",
    "    # Diccionario final\n",
    "    dic_final = {\n",
    "        \"pycaret_reg\": {\"modelo\": mejor_modelo_reg,\"parametros\":parametros_reg},\n",
    "        \"pycaret_ts\": {\"modelo\": mejor_modelo_ts,\"parametros\":parametros_ts},\n",
    "        \"prophet\": mejores_parametros_prophet\n",
    "    }\n",
    "    return news, dic_final\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CALLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lineas = df[\"linea\"].unique()\n",
    "validacion_calls = []\n",
    "diccionario_calls = {}\n",
    "\n",
    "for linea in lineas:\n",
    "    print(f\"Procesando línea: {linea}\")\n",
    "    validacion, diccionario = validacion_multiple(df, \"interpolado_real_calls\", linea)\n",
    "    validacion_calls.append(validacion)\n",
    "    diccionario_calls[linea] = diccionario\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validacion_diaria_calls = pd.concat(validacion_calls, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validacion_diaria_calls.to_excel(\"validacion_diaria_calls.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertir_a_serializable(obj):\n",
    "    if hasattr(obj, 'get_params'):\n",
    "        return obj.get_params()\n",
    "    return str(obj) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diccionario_serializable = {k: convertir_a_serializable(v) for k, v in diccionario_calls.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('diccionario_calls.txt', 'w') as file:\n",
    "    json.dump(diccionario_serializable, file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AHT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lineas = df[\"linea\"].unique()\n",
    "validaciones_aht = []\n",
    "diccionarios_aht = {}\n",
    "\n",
    "for linea in lineas:\n",
    "    print(f\"Procesando línea: {linea}\")\n",
    "    validacion_aht, diccionario_aht = validacion_multiple(df, \"interpolado_real_aht\", linea)\n",
    "    validaciones_aht.append(validacion_aht)\n",
    "    diccionarios_aht[linea] = diccionario_aht"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validacion_diaria_aht = pd.concat(validaciones_aht, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diccionario_serializable_aht = {k: convertir_a_serializable(v) for k, v in diccionarios_aht.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validacion_diaria_aht.to_excel(\"validacion_diaria_aht.xlsx\")\n",
    "with open('diccionario_aht.txt', 'w') as file:\n",
    "    json.dump(diccionario_serializable_aht, file, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfm_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
